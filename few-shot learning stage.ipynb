{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T13:58:25.250727Z",
     "iopub.status.busy": "2023-06-21T13:58:25.250193Z",
     "iopub.status.idle": "2023-06-21T13:58:25.425591Z",
     "shell.execute_reply": "2023-06-21T13:58:25.424631Z",
     "shell.execute_reply.started": "2023-06-21T13:58:25.250694Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略所有警告\n",
    "mm = []\n",
    "mm2 = []\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "eps = 1e-6\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(520)\n",
    "############################################# 21\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "#  Few-shot parameters  #\n",
    "parser.add_argument('--data_name', default='miniImageNet', help='miniImageNet| tieredImageNet')\n",
    "parser.add_argument('--method_name', default='KL', help=' Wass | Wass_CMS | KL | KL_CMS | ADM ')\n",
    "parser.add_argument('--mode', default='train', help='train|val|test')\n",
    "parser.add_argument('--outf', default='./results/')\n",
    "parser.add_argument('--workers', type=int, default=0)\n",
    "parser.add_argument('--way_num', type=int, default=5, help='the number of way/class')\n",
    "parser.add_argument('--shot_num', type=int, default=1, help='the number of shot')\n",
    "parser.add_argument('--query_num', type=int, default=16, help='the number of queries')\n",
    "parser.add_argument('--train_num', type=int, default=10, help='pretrain number, default=10')\n",
    "#  Few-shot parameters  #\n",
    "parser.add_argument('--epochs', type=int, default=50, help='the total number of training epoch')\n",
    "parser.add_argument('--start_epoch', default=0, type=int, help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--lr', type=float, default=0.003, help='learning rate, default=0.005')\n",
    "parser.add_argument('--lr2', type=float, default=100, help='learning rate, default=0.005')\n",
    "parser.add_argument('--adam', action='store_true', default=True, help='use adam optimizer')\n",
    "parser.add_argument('--batch-size', type=int, default=128)\n",
    "parser.add_argument('--print_freq', '-p', default=50, type=int, metavar='N', help='print frequency (default: 100)')\n",
    "parser.add_argument('-f', type=str, default=\"读取额外的参数\")\n",
    "parser.add_argument('--freeze-layers', type=bool, default=False)\n",
    "# 不要改该参数，系统会自动分配\n",
    "parser.add_argument('--device', default='cuda', help='device id (i.e. 0 or 0,1 or cpu)')\n",
    "# 开启的进程数(注意不是线程),在单机中指使用GPU的数量\n",
    "parser.add_argument('--world-size', default=4, type=int,\n",
    "                    help='number of distributed processes')\n",
    "parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "################################################################\n",
    "\n",
    "data_dir = \"\"\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[.1, 2.]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x\n",
    "\n",
    "\n",
    "mocoAug = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "#     transforms.RandomApply([\n",
    "#         transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)  # not strengthened\n",
    "#     ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((84, 84)),\n",
    "])\n",
    "\n",
    "\n",
    "supervisedAug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((84, 84)),\n",
    "])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((84, 84)),\n",
    "])\n",
    "\n",
    "def RGB_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "\n",
    "def load_data(csv_path):\n",
    "    data_dict = {}\n",
    "    data_list = []\n",
    "    with open(csv_path) as csv_file:\n",
    "        csv_context = csv.reader(csv_file, delimiter=',')\n",
    "        for line in csv_context:\n",
    "            if csv_context.line_num == 1:\n",
    "                continue\n",
    "            img_name, img_class = line\n",
    "            data_list.append((img_name, img_class))\n",
    "            if img_class in data_dict:\n",
    "                data_dict[img_class].append(img_name)\n",
    "            else:\n",
    "                data_dict[img_class] = []\n",
    "                data_dict[img_class].append(img_name)\n",
    "    class_list = data_dict.keys()\n",
    "    return data_list, data_dict, class_list\n",
    "\n",
    "class FewShotDataSet(Dataset):\n",
    "    def __init__(self, data_dir, phase='train', loader=RGB_loader):\n",
    "        super(FewShotDataSet, self).__init__()\n",
    "        self.loader = loader\n",
    "        self.img_path = data_dir\n",
    "        if phase == 'train':\n",
    "            self.csv_path = \"\"\n",
    "        elif phase == 'val':\n",
    "            self.csv_path = \"\"\n",
    "        else:\n",
    "            self.csv_path = \"\"\n",
    "\n",
    "        self.data_list, self.data_dict, class_list = load_data(self.csv_path)\n",
    "        self.class_list = sorted(list(class_list))\n",
    "        self.label2Int = {item: idx for idx, item in enumerate(self.class_list)}\n",
    "        self.num_cats = len(self.class_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_item, class_name = self.data_list[item]\n",
    "        label = self.label2Int[class_name]\n",
    "        fn = os.path.join(self.img_path, img_item)\n",
    "        img = self.loader(fn)\n",
    "        img = torch.cat((mocoAug(img).unsqueeze(0), mocoAug(img).unsqueeze(0), supervisedAug(img).unsqueeze(0)), dim=0)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.data_list))\n",
    "\n",
    "def get_dataloader(opt, mode):\n",
    "    dataset = FewShotDataSet(data_dir, phase=mode)\n",
    "    if mode == 'train':\n",
    "        loader = MetaDataloader(dataset, opt, mode)     # opt.episode_train_num默认值10000\n",
    "    elif mode == 'val':\n",
    "        loader = MetaDataloader(dataset, opt, mode)\n",
    "    elif mode == 'test':\n",
    "        loader = MetaDataloader(dataset, opt, mode)\n",
    "    else:\n",
    "        raise ValueError('Mode ought to be in [train, val, test]')\n",
    "    return loader\n",
    "\n",
    "class MetaDataloader(object):\n",
    "    def __init__(self, dataset, opt, mode):\n",
    "        self.dataset = dataset\n",
    "        self.img_root = dataset.img_path\n",
    "        self.loader = dataset.loader\n",
    "\n",
    "        self.way_num = opt.way_num\n",
    "        self.shot_num = opt.shot_num\n",
    "        self.query_num = opt.query_num\n",
    "        # self.batch_size = opt.batch_size\n",
    "        # self.epoch_size = opt.epoch_size\n",
    "        self.num_workers = int(opt.workers)\n",
    "        # self.current_epoch = opt.current_epoch\n",
    "        if mode == 'train':\n",
    "            self.shuffle = True\n",
    "        else:\n",
    "            self.shuffle = False\n",
    "\n",
    "    def sampleImageIdsFrom(self, cat_id, sample_size=1):  # 根据类id采样某一类下个数为sample_size大小的样本\n",
    "        assert (cat_id in self.dataset.data_dict)\n",
    "        assert (len(self.dataset.data_dict[cat_id]) >= sample_size)\n",
    "        # Note: random.sample samples elements without replacement.\n",
    "        return random.sample(self.dataset.data_dict[cat_id], sample_size)\n",
    "\n",
    "    def sampleCategories(self, sample_size=1):  # 对数据集中的类进行采样\n",
    "        class_list = self.dataset.class_list\n",
    "        assert (len(class_list) >= sample_size)\n",
    "        return random.sample(class_list, sample_size)  # 从class_list中随机获得长度为sample_size的种类\n",
    "\n",
    "    def sampleSupQuery(self, categories, query_num, shot_num):\n",
    "        if len(categories) == 0:\n",
    "            return [], []\n",
    "        nCategories = len(categories)\n",
    "        Query_imgs = []\n",
    "        Support_imgs = []\n",
    "\n",
    "        for idx in range(len(categories)):\n",
    "            img_ids = self.sampleImageIdsFrom(\n",
    "                categories[idx],\n",
    "                sample_size=(query_num + shot_num)\n",
    "            )\n",
    "            imgs_novel = img_ids[:query_num]\n",
    "            imgs_exemplar = img_ids[query_num:]\n",
    "\n",
    "            Query_imgs += [(img_id, idx) for img_id in imgs_novel]\n",
    "            Support_imgs += [(img_id, idx) for img_id in imgs_exemplar]\n",
    "\n",
    "        assert (len(Query_imgs) == nCategories * query_num)\n",
    "        assert (len(Support_imgs) == nCategories * shot_num)\n",
    "\n",
    "        return Query_imgs, Support_imgs\n",
    "\n",
    "    def sampleEpisode(self):\n",
    "        categories = self.sampleCategories(self.way_num)\n",
    "        Query_imgs, Support_imgs = self.sampleSupQuery(categories, self.query_num, self.shot_num)\n",
    "        return Query_imgs, Support_imgs\n",
    "\n",
    "    def createExamplesTensorData(self, examples):\n",
    "        images = torch.stack(\n",
    "            [trans_val(self.loader(os.path.join(self.img_root, img_name))) for img_name, label in examples], dim=0)\n",
    "        labels = torch.tensor([label for _, label in examples])\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def load_function(self, iter_idx):\n",
    "        Query_imgs, Support_imgs = self.sampleEpisode()\n",
    "        Xt, Yt = self.createExamplesTensorData(Query_imgs)\n",
    "        Xe, Ye = self.createExamplesTensorData(Support_imgs)\n",
    "        return Xt, Yt, Xe, Ye\n",
    "\n",
    "    def get_iterator(self, index):\n",
    "        rand_seed = index\n",
    "        random.seed(rand_seed)\n",
    "        np.random.seed(rand_seed)\n",
    "        Xt, Yt, Xe, Ye = self.load_function(index)\n",
    "\n",
    "        return Xt, Yt, Xe, Ye\n",
    "\n",
    "    def __call__(self, index):\n",
    "        return self.get_iterator(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "#######################################################################################\n",
    "\n",
    "def conv3x3(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "class Conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = conv3x3(out_channels, out_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet12(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResNet12, self).__init__()\n",
    "\n",
    "        self.inplanes = 3\n",
    "\n",
    "        self.layer1 = self._make_layer(channels[0])\n",
    "        self.layer2 = self._make_layer(channels[1])\n",
    "        self.layer3 = self._make_layer(channels[2])\n",
    "        self.layer4 = self._make_layer(channels[3])\n",
    "\n",
    "        self.out_dims = channels[3]\n",
    "\n",
    "    def _make_layer(self, planes):\n",
    "        downsample = nn.Sequential(\n",
    "            conv1x1(self.inplanes, planes),\n",
    "            nn.BatchNorm2d(planes),\n",
    "        )\n",
    "        block = ResBlock(self.inplanes, planes, downsample)\n",
    "        self.inplanes = planes\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "def resnet12():\n",
    "    return ResNet12([64, 128, 256, 512])\n",
    "\n",
    "def resnet12_wide():\n",
    "    return ResNet12([64, 160, 320, 640])\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)  # 网络初始化，normal_实现基于正态分布的初始化参数\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def get_model(pre_train=False, model_dir=None, num_class=80, dim=128):\n",
    "    model = featureAugNet(num_class, dim)\n",
    "    #     model.apply(weights_init_normal)\n",
    "    if pre_train:\n",
    "        model.load_state_dict(torch.load(model_dir)['state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseEncoder, self).__init__()\n",
    "        self.extractor = resnet12_wide()\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=5, stride=5)\n",
    "        self.linear = nn.Linear(640, 128)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extractor(x)\n",
    "        feature_pool = self.pooling(x)\n",
    "        x = feature_pool.view(feature_pool.size(0), -1)\n",
    "        out = self.act(self.linear(x))\n",
    "        return x, out\n",
    "    \n",
    "class BaseEncoderClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseEncoderClass, self).__init__()\n",
    "        self.extractor = resnet12_wide()\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=5, stride=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extractor(x)\n",
    "        feature_pool = self.pooling(x)\n",
    "        x = feature_pool.view(feature_pool.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class distLinear(nn.Module):\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(distLinear, self).__init__()\n",
    "        self.L = nn.Linear(indim, outdim, bias=False)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        self.L.weight.data = nn.functional.normalize(self.L.weight.data, dim=1)\n",
    "        return 10*self.L(x)\n",
    "    \n",
    "class linearC(nn.Module):\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(linearC, self).__init__()\n",
    "        self.L = nn.Linear(indim, outdim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.L(x)\n",
    "    \n",
    "class MoCo(nn.Module):\n",
    "    def __init__(self, base_encoder, dim=128, K=2048, m=0.999, T=0.1):\n",
    "        super(MoCo, self).__init__()\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.m = m\n",
    "\n",
    "        self.encoder_q = base_encoder()\n",
    "        self.encoder_k = base_encoder()\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)\n",
    "            param_k.requires_grad = False\n",
    "\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        keys = concat_all_gather(keys)\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_ddp(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(batch_size_all).to(device)\n",
    "\n",
    "        # broadcast to all gpus\n",
    "        torch.distributed.broadcast(idx_shuffle, src=0)\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        # shuffled index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this], idx_unshuffle\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_ddp(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # restored index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this]\n",
    "        \n",
    "    def forward(self, im_q, im_k):\n",
    "        q_high, q = self.encoder_q(im_q)\n",
    "        q = nn.functional.normalize(q, dim=1)\n",
    "        # feature = q.clone()\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update_key_encoder()\n",
    "            im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)\n",
    "            _, k = self.encoder_k(im_k)\n",
    "            k = nn.functional.normalize(k, dim=1)\n",
    "            k = self._batch_unshuffle_ddp(k, idx_unshuffle)\n",
    "\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        logits /= self.T\n",
    "        self._dequeue_and_enqueue(k)\n",
    "\n",
    "        return q_high, logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def concat_all_gather(tensor):\n",
    "    \"\"\"\n",
    "    Performs all_gather operation on the provided tensors.\n",
    "    *** Warning ***: torch.distributed.all_gather has no gradient.\n",
    "    \"\"\"\n",
    "    tensors_gather = [torch.ones_like(tensor)\n",
    "        for _ in range(torch.distributed.get_world_size())]\n",
    "    torch.distributed.all_gather(tensors_gather, tensor.contiguous(), async_op=False)\n",
    "\n",
    "    output = torch.cat(tensors_gather, dim=0)\n",
    "    return output   \n",
    "\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "class classFeatureExtractor(nn.Module):\n",
    "    def __init__(self, base_enceoder, num_classes=64, dim=640):\n",
    "        super(classFeatureExtractor, self).__init__()\n",
    "        self.base_encoder = base_enceoder()\n",
    "        self.classifier = distLinear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_encoder(x)\n",
    "        x = nn.functional.normalize(x)\n",
    "        out = self.classifier(x)\n",
    "        return x, out\n",
    "\n",
    "    \n",
    "class featureAugNet(nn.Module):\n",
    "    def __init__(self, num_classes=64, dim=640):\n",
    "        super(featureAugNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.instFeatExt = MoCo(BaseEncoder, dim=128, K=2048, m=0.999, T=0.1)\n",
    "        self.classFeatExt = classFeatureExtractor(BaseEncoderClass, num_classes=num_classes, dim=dim)\n",
    "        self.classifier = distLinear(dim, num_classes)\n",
    "   \n",
    "    @torch.no_grad()\n",
    "    def update_classifier(self):\n",
    "        for param_q, param_k in zip(self.classFeatExt.classifier.parameters(), self.classifier.parameters()):\n",
    "            param_k.data = param_q.data\n",
    "    \n",
    "\n",
    "    def forward(self, im_q, im_k, im_s):\n",
    "#         batch_size = labels.shape[0]\n",
    "#         kl_loss = torch.tensor(0.).to(device)\n",
    "        q_high, logits_u = self.instFeatExt(im_q, im_k)\n",
    "        q_high = nn.functional.normalize(q_high, dim=1)\n",
    "        self.update_classifier()\n",
    "        logits_us = self.classifier(q_high)\n",
    "        s_high, logits_s = self.classFeatExt(im_s)\n",
    "        return q_high, logits_u, logits_us, s_high, logits_s\n",
    "\n",
    "\n",
    "def MoCoModel():\n",
    "    return MoCo(BaseEncoder, dim=128, K=2048, m=0.999, T=0.1)\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "def adjust_learning_rate(opt, optimizer, epoch, F_txt):\n",
    "    lr = opt.lr * (0.5 ** (epoch // 10))\n",
    "    lr2 = opt.lr2 * (0.5 ** (epoch // 5))\n",
    "    print('learning rate: %f' % lr)\n",
    "    print('learning rate: %f' % lr2)\n",
    "    print('Learning rate: %f' % lr, file=F_txt)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    optimizer.param_groups[1]['lr'] = lr\n",
    "    optimizer.param_groups[2]['lr'] = lr2\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "\ta = [1.0*np.array(data[i]) for i in range(len(data))]\n",
    "\tn = len(a)\n",
    "\tm, se = np.mean(a), scipy.stats.sem(a)\n",
    "\th = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
    "\treturn m, h\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True).mul_(100.0 / batch_size).cpu().detach().numpy()\n",
    "            res.append(correct_k)\n",
    "        return res  # 返回topk的准确率\n",
    "\n",
    "\n",
    "def set_save_path(opt):\n",
    "    \"\"\"\n",
    "    settings of the save path\n",
    "    \"\"\"\n",
    "    opt.outf = 'Shot'\n",
    "\n",
    "    if not os.path.exists(opt.outf):\n",
    "        os.makedirs(opt.outf)\n",
    "\n",
    "\n",
    "    # save the opt and results to txt file\n",
    "    txt_save_path = os.path.join(opt.outf, 'opt_resutls.txt')\n",
    "    F_txt = open(txt_save_path, 'a+')\n",
    "\n",
    "    return opt.outf, F_txt\n",
    "\n",
    "\n",
    "def set_save_test_path(opt, finetune=False):\n",
    "    \"\"\"\n",
    "    Settings of the save path\n",
    "    \"\"\"\n",
    "    if not os.path.exists(opt.outf):\n",
    "        os.makedirs(opt.outf)\n",
    "\n",
    "    # save the opt and results to txt file\n",
    "    if finetune:\n",
    "        txt_save_path = os.path.join(opt.outf, 'Test_Finetune_resutls.txt')\n",
    "    else:\n",
    "        txt_save_path = os.path.join(opt.outf, 'Test_resutls.txt')\n",
    "    F_txt_test = open(txt_save_path, 'a+')\n",
    "\n",
    "    return F_txt_test\n",
    "\n",
    "\n",
    "def get_resume_file(checkpoint_dir, F_txt):\n",
    "    if os.path.isfile(checkpoint_dir):\n",
    "        print(\"=> loading checkpoint '{}'\".format(checkpoint_dir))\n",
    "        print(\"=> loading checkpoint '{}'\".format(checkpoint_dir), file=F_txt)\n",
    "        checkpoint = torch.load(checkpoint_dir)\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(checkpoint_dir, checkpoint['epoch_index']))\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(checkpoint_dir, checkpoint['epoch_index']), file=F_txt)\n",
    "\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(checkpoint_dir))\n",
    "        print(\"=> no checkpoint found at '{}'\".format(checkpoint_dir), file=F_txt)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T13:57:12.066862Z",
     "iopub.status.busy": "2023-06-21T13:57:12.066446Z",
     "iopub.status.idle": "2023-06-21T13:57:12.131384Z",
     "shell.execute_reply": "2023-06-21T13:57:12.130306Z",
     "shell.execute_reply.started": "2023-06-21T13:57:12.066827Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_aug_feats(class_feats, aug_num, classMean, intra_classInfo):   #[intra_mean, intra_std] \n",
    "    ori_num = class_feats.size(0)\n",
    "    similars = torch.einsum('ck,nk->cn', [class_feats, classMean])\n",
    "    select_simi, index = torch.sort(similars, dim=1, descending=True)\n",
    "    select_simi = select_simi[:, :5]\n",
    "    class_similars = torch.einsum('ck,kn->cn', [class_feats, class_feats.t()])\n",
    "\n",
    "    select_simi = 10*select_simi\n",
    "    index = index[:, :5]\n",
    "    weights = nn.functional.softmax(select_simi, dim=1)\n",
    "    aug_feats = torch.zeros((ori_num*aug_num, class_feats.size(1)), dtype=torch.float).to(device)\n",
    "    \n",
    "    for i in range(ori_num):\n",
    "        aug_mean = torch.zeros_like(class_feats[i]).to(device)\n",
    "        aug_var = torch.zeros_like(class_feats[i]).to(device)\n",
    "        for j in range(5):\n",
    "            aug_mean += weights[i, j] * intra_classInfo[0][index[i, j]]\n",
    "            aug_var += weights[i, j] * intra_classInfo[1][index[i, j]]\n",
    "        eps = torch.randn((aug_num, class_feats.size(1)), dtype=torch.float).to(device)\n",
    "        recon_feature = class_feats[i] + aug_var * eps + aug_mean\n",
    "        aug_feats[i*aug_num:(i+1)*aug_num] = recon_feature\n",
    "    aug_feats = nn.functional.normalize(aug_feats, dim=1)\n",
    "    return aug_feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T13:58:59.266105Z",
     "iopub.status.busy": "2023-06-21T13:58:59.265762Z",
     "iopub.status.idle": "2023-06-21T13:58:59.335452Z",
     "shell.execute_reply": "2023-06-21T13:58:59.334401Z",
     "shell.execute_reply.started": "2023-06-21T13:58:59.266076Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_train(classifier, extractor, data, train_num, criterion, optimizer, is_aug, aug_num, classMean, intra_classInfo):\n",
    "    x, y = data[0], data[1]\n",
    "    # loss = torch.tensor(0.).to(device)\n",
    "    with torch.no_grad():\n",
    "        feat, _ = extractor.classFeatExt(x)   \n",
    "#     if is_aug:\n",
    "#         aug_feats = get_aug_feats(feat, aug_num, classMean, intra_classInfo)\n",
    "#         aug_labels = y.unsqueeze(1).expand((y.size(0), aug_num)).reshape(-1)\n",
    "#         feat = torch.cat([feat, aug_feats], dim=0)\n",
    "#         y = torch.cat([y, aug_labels], dim=0)\n",
    "    for i in range(train_num):\n",
    "        logits = classifier(feat)\n",
    "        loss = criterion(logits, y)\n",
    "        if is_aug:\n",
    "            aug_feats = get_aug_feats(feat, aug_num, classMean, intra_classInfo) \n",
    "            aug_labels = y.unsqueeze(1).expand((y.size(0), aug_num)).reshape(-1)\n",
    "            aug_logit = classifier(aug_feats)\n",
    "            aug_loss = criterion(aug_logit, aug_labels)\n",
    "            loss = loss + aug_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "def test(extractor, criterion, epoch, is_aug, aug_num, classMean, intra_classInfo):\n",
    "    top1_val = AverageMeter()\n",
    "    top1_val_no = AverageMeter()\n",
    "    top1_val_n = AverageMeter()\n",
    "    for task_num in range(20):\n",
    "        classifier = distLinear(indim=640, outdim=opt.way_num).to(device)\n",
    "        checkpoint_path = \"initial_weights.pt\"\n",
    "        torch.save(classifier.state_dict(), checkpoint_path)\n",
    "        val_optimizer = optim.Adam(classifier.parameters())\n",
    "        test_loader = get_dataloader(opt, 'test')\n",
    "        X_q, Y_q, X_s, Y_s = test_loader(10*epoch + task_num)     # 0 is rand seed\n",
    "        X_q, Y_q, X_s, Y_s = X_q.to(device), Y_q.to(device), X_s.to(device), Y_s.to(device)\n",
    "        query_nums = Y_q.shape[-1]\n",
    "        classifier.train()\n",
    "#         print(\"############################  Before Train  ################################\")\n",
    "#         print(model.L.weight)\n",
    "        classifier = val_train(classifier, extractor, [X_s, Y_s], 400, criterion, val_optimizer, is_aug, aug_num, classMean, intra_classInfo)\n",
    "#         print(\"############################  After Train  ################################\")False\n",
    "#         print(model.L.weight)\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            feats, _ = extractor.classFeatExt(X_q)\n",
    "            logit = classifier(feats)\n",
    "        prec1 = accuracy(logit, Y_q, topk=(1,))\n",
    "        top1_val.update(prec1[0].item(), query_nums)\n",
    "        top1.update(prec1[0].item(), query_nums)\n",
    "        \n",
    "        print('Eposide-({0}): [{1}/{2}]\\t'\n",
    "              'prec1 is {3}\\t'.format(epoch, task_num, 10, prec1[0].item()))\n",
    "\n",
    "                 \n",
    "    print(\"测试结果为\" + str(top1.avg))\n",
    "    print(\"第%d代验证结果为\"%epoch_item + str(top1_val.avg))\n",
    "    \n",
    "    return top1_val.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T13:59:03.421645Z",
     "iopub.status.busy": "2023-06-21T13:59:03.421290Z",
     "iopub.status.idle": "2023-06-21T14:05:14.008370Z",
     "shell.execute_reply": "2023-06-21T14:05:14.007424Z",
     "shell.execute_reply.started": "2023-06-21T13:59:03.421614Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# plt.axis('off')\n",
    "global best_prec1, epoch_index\n",
    "best_prec1 = 0\n",
    "epoch_index = 0\n",
    "top1 = AverageMeter()\n",
    "\n",
    "batch_size = 128\n",
    "extractor = featureAugNet().to(device)\n",
    "\n",
    "pretrain_path = \"\"\n",
    "pretrain_state_dict = torch.load(pretrain_path, map_location=\"cpu\")['state_dict']\n",
    "pretrain_state_dict = {k[7:]:v for k, v in pretrain_state_dict.items()}\n",
    "# 这里注意，一定要指定map_location参数，否则会导致第一块GPU占用更多资源\n",
    "extractor.load_state_dict(pretrain_state_dict)\n",
    "\n",
    "class_feat = torch.load(\"\", map_location=\"cuda\")\n",
    "instance_feat = torch.load(\"\", map_location=\"cuda\")\n",
    "\n",
    "\n",
    "intra_classMean = (instance_feat-class_feat).mean(dim=1).to(device)\n",
    "intra_classStd = torch.pow((instance_feat-class_feat).var(dim=1), 0.5).to(device)\n",
    "\n",
    "\n",
    "classMean = nn.functional.normalize(class_feat.mean(dim=1), dim=1).to(device)\n",
    "\n",
    "intra_classInfo = [intra_classMean, intra_classStd]\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "print('===================================== Training on the train set =====================================')\n",
    "for epoch_item in range(0, 25):\n",
    "    extractor.eval()\n",
    "    test(extractor, criterion, epoch_item, True, 5, classMean, intra_classInfo)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
